<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Nowgeun's Blog &middot; 
    

  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-icon.png">
  <link rel="shortcut icon" href="/public/favicon-96x96.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  
  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <img src="/assets/images/profile.jpg" width="130" height="130">


    <p>A blog of Jaekeun Lee for recording life and keeping track of studies, interests and experiences</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about.html">About Me</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/archive.html">Archive</a>
        
      
    
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/contact.html">Contact</a>
        
      
    
      
    
      
        
      
    
      
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/tags/">Tags</a>
        
      
    

    <!-- <a class="sidebar-nav-item" href="/archive/v1.0.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a> -->
    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2018. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
                  <a href="/" title="Home">Nowgeun's Blog</a>

                  
                    &nbsp;&nbsp;&nbsp;
                    <small><a href="/about">About Me</a></small>
                  
                    &nbsp;&nbsp;&nbsp;
                    <small><a href="/archive">Archive</a></small>
                  
                    &nbsp;&nbsp;&nbsp;
                    <small><a href="/contact">Contact</a></small>
                  
                    &nbsp;&nbsp;&nbsp;
                    <small><a href="/tags">Tags</a></small>
                  

            
          </h3>
        </div>
      </div>

      <div class="container content">
        
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="//2018/10/12/Spearman_Correlation.html">
        Spearman Correlation
      </a>
    </h1>
    
    <span class="post-date">12 Oct 2018</span>
    <img src="/assets/images/tag-256.png" alt="Tags: " class="tag-img"/>
<div class="post-tags">
    
    
    <a href="/tags/#python3">python3</a>,
    
    <a href="/tags/#statistics">statistics</a>
    
</div>
<br/>

    <br/>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


    <h2 id="correlation">Correlation</h2>

<p>Correlation is a concept that is used to measure the association between variables, whether they are related or not. If the variables are related examining whether the relationship is positive or negative, or whether a specific model can explain the relationship. (Linear, non-linear)</p>

<p> </p>

<p>For example, pearson correlation measure the <strong>“linearity”</strong> of two continuous variables, assuming that the two variables follow normal distribution.</p>

<p> 
 
 </p>

<h3 id="spearman-correlation-rho">Spearman Correlation ($\rho$)</h3>

<ul>
  <li>
    <p>Spearman’s Correlation measures the correlation between two variables, taking their rankings into account. It evaluates the variables in a <strong>non-parametric way</strong>; meaning that it uses statistical method where data is not required to follow a normal distribution. In other words, Spearman correlation <em>does not assume anything about the distribution of data.</em></p>
  </li>
  <li>The assumption required for Spearman correlation are that:
    <ul>
      <li>the data must be at least ordinal</li>
      <li>the scores on one variable must be <strong>monotonically</strong> related to the other variable.</li>
    </ul>

    <p><img src="/assets/images/monotonic.png" /></p>

    <p> </p>
  </li>
  <li>The correlation is calcuated as a following equation: $\rho = 1 - \frac{6 \sum d_{i}^{2}}{n(n^{2}-1)}$
    <ul>
      <li>$\rho$ : Spearman rank correlation</li>
      <li>$d_i$ : the difference between the ranks of corresponding variables</li>
      <li>$n$ : number of observations</li>
    </ul>
  </li>
</ul>

<p> 
 
 </p>

<h4 id="interpreting-the-value-of-spearman-correlation">Interpreting the value of Spearman Correlation</h4>

<ul>
  <li>
    <p>The Spearman correlation coefficient can take values from +1 to -1 
 <script type="math/tex">% <![CDATA[
( -1 < \rho < +1) %]]></script></p>
  </li>
  <li>
    <p>+1 indicates a perfect association of ranks, while -1 indicates a perfect negative association between ranks. The closer value to 0, implies the weaker association between the ranks.</p>
  </li>
</ul>


    <br/>
    <br/>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="//2018/10/11/NaiveBayes.html">
        Naive Bayes Classifier
      </a>
    </h1>
    
    <span class="post-date">11 Oct 2018</span>
    <img src="/assets/images/tag-256.png" alt="Tags: " class="tag-img"/>
<div class="post-tags">
    
    
    <a href="/tags/#statistics">statistics</a>,
    
    <a href="/tags/#bayesian">bayesian</a>,
    
    <a href="/tags/#machine learning">machine learning</a>
    
</div>
<br/>

    <br/>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


    <p> </p>

<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<p> </p>

<p>Naive Bayes is one of the machine learning techniques used for classification and prediction. Obviously, it involves <a href="/2018/10/10/Bayes-Theorem.html">Bayesian Theorem</a>. The classifier assumes that all <strong>explanatory variables are independent to each other</strong> respectively contributing to the response variable.</p>

<h4 id="advantage">Advantage</h4>

<ul>
  <li>
    <p>Naive Bayes classifier can be trained effectively on Supervised Learning Environment, because it does not require a lot of data for training.</p>
  </li>
  <li>
    <p>Despite its simplicity and design, it has been proven to work well in various complex situations</p>
  </li>
</ul>

<p> 
 </p>

<h3 id="model">Model</h3>

<p>Naives Bayes Model is conditional probabilistic model. The <script type="math/tex">n</script> features (independent explanatory variables) are represented as vector <script type="math/tex">\mathbf x = (x_1,x_2,x_3,x_4,....x_n)</script>,  which is the data given for instance classification.</p>

<p>The instance probabilities are <script type="math/tex">p(C_k \vert x_1,...,x_n)</script>. In other words, the probability is calculated for all <script type="math/tex">K</script> (or <script type="math/tex">C_k</script>) possible outcomes.</p>

<p> 
 </p>

<div style="text-align:center">Instance Probability Model</div>

<script type="math/tex; mode=display">p(C_k \vert \mathbf x) \biggl(= p(C_k \vert x_1,...,x_n)\biggr) = {p(C_k)*p(\mathbf x \vert C_k) \over p(\mathbf x)}</script>

<p> 
 
 </p>

<p>The denominator $p(\mathbf x)$ is always the same, so when comparing probability between instances and find the best classification, the numerator part is the one that only matters.</p>

<p>Using the characteristics of conditional probability and if the explanatory variables are independent, the numerator part above can be altered like this:</p>
<div style="text-align:center">.</div>
<div style="text-align:center">.</div>
<div style="text-align:center">.</div>
<p> 
 </p>

<div style="text-align:center">Altered Numerator</div>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(C_k)*p(\mathbf x \vert C_k) \\
& = p(C_k)*p(x_1,...,x_n \vert C_k)\\

& = p(x_1,...,x_n,C_k)\\

& = p(x_1 \vert x_2,..,x_n,C_k)*p(x_2,...,x_n,C_k)\\
  
& = p(x_1 \vert x_2,..,x_n)*p(x_2 \vert x_3,...,
  x_n)*p(x_3,...,x_n,C_k)\\
  
& = ...\\
  
& = p(x_1 \vert x_2,..,x_n,C_k)*p(x_2 \vert x_3,...,x_n,C_k)*p(x_3 \vert x_4,...,x_n,C_k)...*p(x_{n} \vert C_k)*p(C_k)\\
  
& = p(x_1 \vert C_k)*p(x_2 \vert C_k)*p(x_3 \vert C_k)...*p(x_{n-1} \vert C_k)*p(x_n \vert C_k)\\
  \end{align} %]]></script>

<p>Thus, the final model can be written as:</p>

<script type="math/tex; mode=display">p(C_k \vert \mathbf x) \varpropto p(C_k)*\prod_{i=1}^{n} p(x_i \vert C_k)</script>

<p>In short, the Naive Bayes Classifier is product of all conditional probablities of explanatory variables to the given category and the probability of category itself.</p>


    <br/>
    <br/>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="//2018/10/10/Bayes-Theorem.html">
        Bayes' Probability
      </a>
    </h1>
    
    <span class="post-date">10 Oct 2018</span>
    <img src="/assets/images/tag-256.png" alt="Tags: " class="tag-img"/>
<div class="post-tags">
    
    
    <a href="/tags/#statistics">statistics</a>,
    
    <a href="/tags/#bayesian">bayesian</a>,
    
    <a href="/tags/#probability">probability</a>
    
</div>
<br/>

    <br/>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


    <p> </p>

<h2 id="bayes-theory">Bayes’ Theory</h2>
<p> </p>

<p>In probabilistic study, there are two main stream approaches:</p>

<ol>
  <li>Frequentist Approach( <em>the classic probability</em> )
    <ul>
      <li>Define probability as an event’s relative frequency in a large number of trials when performed infinite times</li>
      <li>
        <script type="math/tex; mode=display">P(x) = \lim_{n_t -> \infty} \frac{n_x}{n_t}</script>
      </li>
      <li><script type="math/tex">n_t</script> is total number of trials and <script type="math/tex">n_x</script> is number of that event <script type="math/tex">x</script> occurred and <script type="math/tex">P(x)</script> is the probability</li>
      <li>However, application of frequentist approach is nearly impossible in the real word, because you cannot simply try everything infinite amount of times. It is difficult to apply real world problems…</li>
    </ul>
  </li>
</ol>

<p> 
 </p>

<ol>
  <li>Bayesian Approach
    <ul>
      <li>Define probability as a <strong>“degree of belief”</strong>. It is subjective, but not random</li>
      <li>Experience and data are used to <strong>update</strong> the probability</li>
      <li>
        <p>Practical to apply in real world problems!!</p>
      </li>
      <li>
        <p>Bayesian Theorem: 
 <script type="math/tex">P(H|E) = {P(H \bigcap E) \over P(E)} = \frac{P(E|H)*P(H)}{P(E)} =  \frac{P(E|H)*P(H)}{P(H)*P(E|H) + P(H^c)*P(E|H^c)}</script></p>

        <ul>
          <li>
            <p><script type="math/tex">P(H)</script> is called the prior probability, the probability that you know before the “update”</p>
          </li>
          <li>
            <p><script type="math/tex">P(H \vert E)</script> is called the posterior probability, the updated probability with new information</p>
          </li>
          <li>
            <p>The Bayesian Theorem uses conditional probability to update prior probability to posterior probability</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p> 
 
 
 </p>

<blockquote>
  <h4 id="conditional-probability">Conditional Probability</h4>

  <p><script type="math/tex">P(A \vert B) = {P(A \cap B) \over P(B)}</script></p>
  <ul>
    <li>Given that event B occurred, the chance that <script type="math/tex">P(A)</script> also occurred. (probability of A given B)</li>
    <li>if the two events A and B are completely independent,  <script type="math/tex">P(A \vert B) = P(A)</script></li>
    <li>This implies that information of B is irrelevant or useless when you want to know A</li>
  </ul>
</blockquote>

<p> 
 </p>

<h2 id="example">Example</h2>

<p>Let’s say that a mother is in her 40s. She got a positive reaction from X-ray examination of breast cancer. What is her probability of really having a breast cancer?</p>

<ul>
  <li>Probability that women in her 40s will have a breast cancer is 1%</li>
  <li>Probability that cancer patient of women in 40s will be diagnosed positive from X-ray examination is 90%</li>
  <li>Probability that healthy women in 40s will be diagnosed positive from X-ray examination is 5%</li>
</ul>

<blockquote>
  <h4 id="solution">Solution</h4>
  <p>The probability that we want to know is <script type="math/tex">P( c \vert p)</script></p>

  <p>Given informations are:</p>
  <ul>
    <li><script type="math/tex">P(c)</script> = 0.01</li>
    <li><script type="math/tex">P(p \vert c)</script> = 0.9</li>
    <li><script type="math/tex">P (p \vert c^c)</script> = 0.05</li>
  </ul>

  <p>Using the Bayes theorem:
<script type="math/tex">P(c \vert p) = {P(c)*P(p \vert c) \over P(p)}</script></p>

  <p>So all we need to know is the value of <script type="math/tex">P(p)</script> which is:</p>
  <h5 id="pp--ppcpc--ppccpcc--09001--005099--00585"><script type="math/tex">P(p) = P(p|c)*P(c) + P(p|c^c)*P(c^c) = 0.9*0.01 + 0.05*0.99 = 0.0585</script></h5>

  <p>The final calculation of what we want to know would be: 
0.01 * 0.9 / 0.0585 = 0.15384…</p>

  <p>Thus, the probability that a mother will have breast cancer, given that her X-ray examination was positive is about 15%</p>
</blockquote>

    <br/>
    <br/>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="//2018/10/08/LDA-topic-modeling.html">
        Topic Modeling and Latent Dirichlet Allocation
      </a>
    </h1>
    
    <span class="post-date">08 Oct 2018</span>
    <img src="/assets/images/tag-256.png" alt="Tags: " class="tag-img"/>
<div class="post-tags">
    
    
    <a href="/tags/#lda">lda</a>,
    
    <a href="/tags/#topic modeling">topic modeling</a>,
    
    <a href="/tags/#nlp">nlp</a>
    
</div>
<br/>

    <br/>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


    <p> </p>

<h2 id="topic-modeling">Topic Modeling</h2>
<p> </p>

<p>The objective of topic modeling is very self explanatory; discovering abstract <strong>“topics”</strong> that can most describe semantic meaning of documents. It is an integrated field of machine learning and natural language processing, and a frequently used text-mining tool to discover hidden semantic structures in the texts. Topic modeling can help facilitating organization of vast amount of documents and find insights from unstructured text data.</p>

<p> 
 </p>

<h2 id="lda-latent-dirichlet-allocation">LDA (Latent Dirichlet Allocation)</h2>

<p> </p>

<p>LDA is one of the graphical models used for topic modeling. LDA is a generative statistical model that posits specific probability of word appearance in accordance to a specific topic. The image below best explains how LDA works.</p>

<p align="center">
<img src="/assets/images/lda_diagram.png" />
</p>

<p> </p>

<p>The key part of LDA lies in the right part of the diagram, “Topic proportions and Assignments”. LDA views documents as a mixture of various topics and each topic consists of a distribution of words.  LDA has several assumptions:</p>

<ul>
  <li>number of <em>N</em> words are decided by <strong><em>Poisson distribution</em></strong></li>
  <li>from number <em>K</em> topic sets, document topics are decided by <strong><em>Dirichlet distribution</em></strong></li>
  <li>each word <script type="math/tex">w_{i}</script> in the document is generated by following rules:
    <ol>
      <li>pick a topic in accordance to the multinomial distribution sampled above</li>
      <li>generate the word using the topic in accordance to the multinomial distribution of the words in that topic</li>
    </ol>
  </li>
</ul>

<p> </p>

<h4 id="model">Model</h4>

<p> </p>

<p align="center">
<img src="/assets/images/Smoothed_LDA.png" />
</p>

<p> </p>

<ul>
  <li><script type="math/tex">\alpha</script> is the parameter of the Dirichlet prior on the per-document topic distributions,</li>
  <li><script type="math/tex">\beta</script> is the parameter of the Dirichlet prior on the per-topic word distribution,</li>
  <li><script type="math/tex">\theta_{m}</script> is the topic distribution for document <script type="math/tex">m</script>,</li>
  <li><script type="math/tex">\varphi_{k}</script> is the word distribution for topic <script type="math/tex">k</script>,</li>
  <li><script type="math/tex">z_{mn}</script> is the topic for the <script type="math/tex">n</script>-th word in document <script type="math/tex">m</script>, and</li>
  <li><script type="math/tex">w_{mn}</script> is the specific word.</li>
</ul>


    <br/>
    <br/>
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="//2018/10/07/TF-IDF.html">
        TF-IDF
      </a>
    </h1>
    
    <span class="post-date">07 Oct 2018</span>
    <img src="/assets/images/tag-256.png" alt="Tags: " class="tag-img"/>
<div class="post-tags">
    
    
    <a href="/tags/#python3">python3</a>,
    
    <a href="/tags/#nlp">nlp</a>,
    
    <a href="/tags/#tf-idf">tf-idf</a>
    
</div>
<br/>

    <br/>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                       }
                    });
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
    </script>
<script
    type="text/javascript"
    charset="utf-8"
    src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
    >
    </script>


    <h2 id="tf-and-idf">TF and IDF</h2>

<p>Text Frequency (TF)</p>

<ul>
  <li>TF is an index that shows frequency of words in each document in the corpus. It is simply calculated by the ratio of word counts by the total number of words in that document. <strong>Each word has its own TF value</strong></li>
</ul>

<p> </p>

<p>Inverse Document Freqeucny (IDF)</p>

<ul>
  <li>IDF is an index that shows the relative weight of words across all documents in the corpus. In other words, it is a representation of rarity of a word in the set of documents. <strong>Each word has its own IDF value</strong></li>
</ul>

<p> 
 </p>

<h2 id="tf-idf">TF-IDF</h2>

<ul>
  <li>TF-IDF is a multiplication of TF and IDF values. It is a numerical statistics that aims to reflect significance of word in a particular document (<em>TF</em>), also considering other documents in the group (<em>IDF</em>).</li>
</ul>

<p> 
 </p>

<h2 id="preprocessing">Preprocessing</h2>

<ul>
  <li>Before you calculate the TF-IDF of all words, each documents need to be processed. They need to be <strong>tokenized</strong></li>
  <li><strong>Tokenizing</strong> is process of classifying sections of string and parsing them.</li>
  <li>For example, the text “He is a good boy” can be tokenized into: [“He”,”is”,”a”,”good”,”boy”]</li>
  <li>The processing methods can vary depending on stemming or lemmatization methods</li>
</ul>

<h5 id="example-code">Example code</h5>

<p> </p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># dependencies
import os
import nltk

work_dir = "/Users/nowgeun/Desktop/Research/Documents/"
text_files = os.listdir(work_dir)


def pre_processing(text):
    # lowercase
    text=text.lower()
    
    # remove tags
    text=re.sub("&amp;lt;/?.*?&amp;gt;"," &amp;lt;&amp;gt; ",text)
    
    # remove special characters and digits
    text=re.sub("(\\d|\\W)+"," ",text)
    
    return text
    

def tokenize(text):
   tokens = nltk.word_tokenize(text)
    stems = []
    
    #pos(part of speech) tagging: grammatical tagging of word by their category (verb,noun,etc)
    for item in nltk.pos_tag(tokens):
        # Filtering tokens with only Nouns (N)
        if item[1].startswith("N"):
            if len(nltk.wordnet.WordNetLemmatizer().lemmatize(item[0])) == 1:
                pass
            else:
                stems.append(nltk.wordnet.WordNetLemmatizer().lemmatize(item[0]))

    return stems

# creating corpus from text files using preprocessing function 

token_dict = {}

for txt in text_files:
    if txt.endswith(".txt"):
        with open (work_dir+ txt) as f:
        
            data = "".join(f.readlines()).replace("\n"," ")
            data = pre_processing(data)

            token_dict[txt] = data &amp;nbsp; &amp;nbsp;
</code></pre></div></div>

<h3 id="tf-idf-computation-using-scikit-learn-package">TF-IDF computation using Scikit-Learn package</h3>

<p> </p>

<h5 id="example-code-1">Example code</h5>

<p> </p>

<p>With Scikit-Learn package, you can compute the tf-idf value and retrieve the results as a matrix form.</p>
<ul>
  <li>Each row of matrix represents respective documents</li>
  <li>Each column of matrix represents words that appear on all documents</li>
  <li>The matrix is sparse-matrix (where most values are 0). This is because not all words appear on all documents or the frequency of the word itself is very low</li>
</ul>

<p> </p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')

# matrix of all documents in rows and idf values of respective words in column
matrix = vectorizer.fit_transform(token_dict.values())
</code></pre></div></div>


    <br/>
    <br/>
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>


 
        

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
